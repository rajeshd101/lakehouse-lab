services:
  # Astronomer/Airflow stack
  postgres-airflow:
    container_name: lakehouse-rjd-airflow-postgres
    image: postgres:15
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - ./DWH/airflow/db:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    networks:
      - lakehouse-net

  airflow-webserver:
    container_name: lakehouse-rjd-airflow-webserver
    build: .
    command: airflow api-server
    depends_on:
      postgres-airflow:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS: admin:ADMIN
      AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE: /usr/local/airflow/simple_auth_manager_passwords.json
      AIRFLOW__CORE__AUTH_MANAGER: airflow.api_fastapi.auth.managers.simple.simple_auth_manager.SimpleAuthManager
      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: http://airflow-webserver:8080/execution/
      AIRFLOW__API__SECRET_KEY: "local-dev-api-secret"
      AIRFLOW__API_AUTH__JWT_SECRET: "local-dev-jwt-secret"
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin
      TRINO_HOST: trino
      TRINO_PORT: "8080"
      TRINO_USER: admin
      TRINO_CATALOG: iceberg
      SCHEMA: ${SCHEMA}
    volumes:
      - ./dags:/usr/local/airflow/dags
      - ./include:/usr/local/airflow/include
      - ./plugins:/usr/local/airflow/plugins
      - ./DWH/airflow/logs:/usr/local/airflow/logs
      - ./simple_auth_manager_passwords.json:/usr/local/airflow/simple_auth_manager_passwords.json
      - ./dbt_project:/usr/local/airflow/dbt_project
    ports:
      - "8080:8080"
    restart: unless-stopped
    networks:
      - lakehouse-net

  airflow-scheduler:
    container_name: lakehouse-rjd-airflow-scheduler
    build: .
    command: airflow scheduler
    depends_on:
      postgres-airflow:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS: admin:ADMIN
      AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE: /usr/local/airflow/simple_auth_manager_passwords.json
      AIRFLOW__CORE__AUTH_MANAGER: airflow.api_fastapi.auth.managers.simple.simple_auth_manager.SimpleAuthManager
      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: http://airflow-webserver:8080/execution/
      AIRFLOW__API__SECRET_KEY: "local-dev-api-secret"
      AIRFLOW__API_AUTH__JWT_SECRET: "local-dev-jwt-secret"
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      TRINO_HOST: trino
      TRINO_PORT: "8080"
      TRINO_USER: admin
      TRINO_CATALOG: iceberg
      SCHEMA: ${SCHEMA}
    volumes:
      - ./dags:/usr/local/airflow/dags
      - ./include:/usr/local/airflow/include
      - ./plugins:/usr/local/airflow/plugins
      - ./DWH/airflow/logs:/usr/local/airflow/logs
      - ./simple_auth_manager_passwords.json:/usr/local/airflow/simple_auth_manager_passwords.json
      - ./dbt_project:/usr/local/airflow/dbt_project
    restart: unless-stopped
    networks:
      - lakehouse-net

  airflow-dag-processor:
    container_name: lakehouse-rjd-airflow-dag-processor
    build: .
    command: airflow dag-processor
    depends_on:
      postgres-airflow:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS: admin:ADMIN
      AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE: /usr/local/airflow/simple_auth_manager_passwords.json
      AIRFLOW__CORE__AUTH_MANAGER: airflow.api_fastapi.auth.managers.simple.simple_auth_manager.SimpleAuthManager
      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: http://airflow-webserver:8080/execution/
      AIRFLOW__API__SECRET_KEY: "local-dev-api-secret"
      AIRFLOW__API_AUTH__JWT_SECRET: "local-dev-jwt-secret"
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
      TRINO_HOST: trino
      TRINO_PORT: "8080"
      TRINO_USER: admin
      TRINO_CATALOG: iceberg
      SCHEMA: ${SCHEMA}
    volumes:
      - ./dags:/usr/local/airflow/dags
      - ./include:/usr/local/airflow/include
      - ./plugins:/usr/local/airflow/plugins
      - ./DWH/airflow/logs:/usr/local/airflow/logs
      - ./simple_auth_manager_passwords.json:/usr/local/airflow/simple_auth_manager_passwords.json
      - ./dbt_project:/usr/local/airflow/dbt_project
    restart: unless-stopped
    networks:
      - lakehouse-net

  airflow-init:
    container_name: lakehouse-rjd-airflow-init
    build: .
    command: airflow db migrate
    depends_on:
      postgres-airflow:
        condition: service_healthy
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow:5432/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_USERS: admin:ADMIN
      AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_PASSWORDS_FILE: /usr/local/airflow/simple_auth_manager_passwords.json
      AIRFLOW__CORE__AUTH_MANAGER: airflow.api_fastapi.auth.managers.simple.simple_auth_manager.SimpleAuthManager
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY}
    volumes:
      - ./dags:/usr/local/airflow/dags
      - ./include:/usr/local/airflow/include
      - ./plugins:/usr/local/airflow/plugins
      - ./DWH/airflow/logs:/usr/local/airflow/logs
      - ./simple_auth_manager_passwords.json:/usr/local/airflow/simple_auth_manager_passwords.json
    networks:
      - lakehouse-net

  # Trino - central SQL engine
  trino:
    container_name: lakehouse-rjd-airflow-trino
    image: trinodb/trino:451
    ports:
      - "9080:8080"
    entrypoint:
      - /bin/sh
      - -c
      - |
        # Manage catalogs based on .env flags
        # We use temporary copies to avoid deleting files on the host volume
        mkdir -p /tmp/trino/catalog
        cp /etc/trino/catalog/*.properties /tmp/trino/catalog/
        
        if [ "$$ENABLE_ICEBERG" != "true" ]; then
          rm -f /tmp/trino/catalog/iceberg.properties
        fi
        if [ "$$ENABLE_SQLSERVER" != "true" ]; then
          rm -f /tmp/trino/catalog/sqlserver.properties
        fi
        if [ "$$ENABLE_ORACLE" != "true" ]; then
          rm -f /tmp/trino/catalog/oracle.properties
        fi
        if [ "$$ENABLE_SNOWFLAKE" != "true" ]; then
          rm -f /tmp/trino/catalog/snowflake.properties
        fi
        if [ "$$ENABLE_DATABRICKS" != "true" ]; then
          rm -f /tmp/trino/catalog/databricks.properties
        fi
        if [ "$$ENABLE_AWS" != "true" ]; then
          rm -f /tmp/trino/catalog/aws.properties
        fi
        if [ "$$ENABLE_GCP" != "true" ]; then
          rm -f /tmp/trino/catalog/gcp.properties
        fi
        if [ "$$ENABLE_AZURE" != "true" ]; then
          rm -f /tmp/trino/catalog/azure.properties
        fi
        
        # Run Trino with the filtered catalogs
        /usr/lib/trino/bin/run-trino --etc-dir /etc/trino --data-dir /data/trino -Dcatalog.config-dir=/tmp/trino/catalog
    volumes:
      - ./trino/catalog:/etc/trino/catalog
      - ./DWH/iceberg:/iceberg/warehouse
      - ./DWH/trino/data:/data/trino
    environment:
      - SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}
      - SNOWFLAKE_USER=${SNOWFLAKE_USER}
      - SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}
      - SNOWFLAKE_ROLE=${SNOWFLAKE_ROLE}
      - SNOWFLAKE_WAREHOUSE=${SNOWFLAKE_WAREHOUSE}
      - SNOWFLAKE_DATABASE=${SNOWFLAKE_DATABASE}
      - DATABRICKS_HOST=${DATABRICKS_HOST}
      - DATABRICKS_HTTP_PATH=${DATABRICKS_HTTP_PATH}
      - DATABRICKS_TOKEN=${DATABRICKS_TOKEN}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_REGION=${AWS_REGION}
      - ENABLE_ICEBERG=${ENABLE_ICEBERG}
      - ENABLE_SQLSERVER=${ENABLE_SQLSERVER}
      - ENABLE_ORACLE=${ENABLE_ORACLE}
      - ENABLE_SNOWFLAKE=${ENABLE_SNOWFLAKE}
      - ENABLE_DATABRICKS=${ENABLE_DATABRICKS}
      - ENABLE_AWS=${ENABLE_AWS}
      - ENABLE_GCP=${ENABLE_GCP}
      - ENABLE_AZURE=${ENABLE_AZURE}
    networks:
      - lakehouse-net
    depends_on:
      - hive-metastore
    restart: unless-stopped

  # Hive Metastore for Iceberg (public image)
  hive-metastore-db:
    container_name: lakehouse-rjd-airflow-hive-metastore-db
    image: postgres:11
    environment:
      POSTGRES_DB: metastore
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
    volumes:
      - ./DWH/hive-metastore/db:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "hive", "-d", "metastore"]
      interval: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - lakehouse-net

  hive-metastore:
    container_name: lakehouse-rjd-airflow-hive-metastore
    image: apache/hive:3.1.3
    depends_on:
      hive-metastore-db:
        condition: service_healthy
    ports:
      - "9083:9083"
    environment:
      SERVICE_NAME: metastore
      DB_DRIVER: postgres
      IS_RESUME: "true"
      SERVICE_OPTS: "-Djavax.jdo.option.ConnectionURL=jdbc:postgresql://hive-metastore-db:5432/metastore -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=hive"
      HIVE_SITE_CONF_javax_jdo_option_ConnectionPassword: hive
      HIVE_SITE_CONF_hive_metastore_warehouse_dir: /iceberg/warehouse
    volumes:
      - ./DWH/iceberg:/iceberg/warehouse
    restart: unless-stopped
    networks:
      - lakehouse-net

  # Spark - processing engine
  spark-master:
    container_name: lakehouse-rjd-airflow-spark-master
    image: apache/spark:3.5.1
    user: root
    entrypoint: ["/bin/bash", "-c"]
    command: 
      - |
        /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master &
        sleep 5;
        /opt/spark/sbin/start-thriftserver.sh \
          --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0 \
          --hiveconf hive.server2.thrift.port=10001 \
          --hiveconf hive.server2.thrift.bind.host=0.0.0.0 \
          --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \
          --conf spark.sql.catalog.spark_catalog.type=hadoop \
          --conf spark.sql.catalog.spark_catalog.warehouse=/warehouse \
          --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions;
        tail -f /opt/spark/logs/*
    ports:
      - "7077:7077"
      - "4040:4040"
      - "10001:10001"
    volumes:
      - ./spark/conf:/opt/spark/conf
      - ./DWH/iceberg:/warehouse
    restart: unless-stopped
    networks:
      - lakehouse-net

  spark-worker:
    container_name: lakehouse-rjd-airflow-spark-worker
    image: apache/spark:3.5.1
    user: root
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    volumes:
      - ./spark/conf:/opt/spark/conf
      - ./DWH/iceberg:/warehouse
    restart: unless-stopped
    networks:
      - lakehouse-net

  # SQL Server - relational source
  sqlserver:
    container_name: lakehouse-rjd-sqlserver
    image: mcr.microsoft.com/mssql/server:2022-latest
    environment:
      - ACCEPT_EULA=Y
      - MSSQL_SA_PASSWORD=${MSSQL_SA_PASSWORD}
    ports:
      - "1433:1433"
    volumes:
      - ./DWH/sqlserver/data:/var/opt/mssql/data
    networks:
      - lakehouse-net
    restart: unless-stopped

  # Oracle - relational source
  oracle:
    container_name: lakehouse-rjd-oracle
    image: gvenzl/oracle-free:latest
    environment:
      - ORACLE_PASSWORD=${ORACLE_PASSWORD}
    ports:
      - "1521:1521"
    volumes:
      - ./DWH/oracle/data:/opt/oracle/oradata
    networks:
      - lakehouse-net
    restart: unless-stopped

  # Query UI - Custom SQL Editor
  query-ui:
    container_name: lakehouse-rjd-query-ui
    build: ./query_ui
    ports:
      - "5001:5000"
    environment:
      - TRINO_HOST=trino
      - TRINO_PORT=8080
      - TRINO_USER=admin
      - TRINO_CATALOG=iceberg
      - SCHEMA=default
    networks:
      - lakehouse-net
    depends_on:
      - trino
    restart: unless-stopped

networks:
  lakehouse-net:
    driver: bridge
